{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML\n",
    "#import sklearn\n",
    "#sklearn.__version__# use this when reopening previously saved ML models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    #pprint allows us to see current hyperparameter values of our model \n",
    "from pprint import pprint\n",
    "# import data\n",
    "fullData = pd.read_csv(\"vminStd.csv\")\n",
    "permanent = pd.read_csv(\"vminStd.csv\")\n",
    "fullData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep rows with Vmin value, and delete unuseful columns\n",
    "fullData.dropna(subset=['Shmoo Value'], inplace=True)\n",
    "fullData.drop([\"VDD (Range)\", \"Library #1\", \"DVDD (Range)\", \"Period (Range)\", \"Result\", \"Test Number\"], axis = 1, inplace = True)\n",
    "##########################################################################################\n",
    "# DROP FILE INDEX COLUMN FOR NOW, ADD THE DATA LATER TO INCLUDE LEAKAGE ETC IN THE MODEL.\n",
    "fullData.drop([\"File Index\"], axis = 1, inplace = True)\n",
    "##########################################################################################\n",
    "\n",
    "# Change strings to integers, get rid of 'V' for voltages, etc...\n",
    "fullData['Shmoo Value'] = fullData['Shmoo Value'].map(lambda x: x.rstrip('V'))\n",
    "fullData[\"Shmoo Value\"] = fullData[\"Shmoo Value\"].astype('float')\n",
    "\n",
    "# Use this to decide between one-hot encoding and categorical encoding (or else)\n",
    "'''print(\"Data Columns and Different Values: \")\n",
    "print(fullData.columns)\n",
    "print()\n",
    "for column in fullData.columns:\n",
    "    print(fullData[str(column)].unique())\n",
    "'''\n",
    "# one hot encode categorical values. See bookmarks for why one hot rather than else\n",
    "fullData = pd.get_dummies(fullData, columns=[\"Chip Type\", \"Library #2\"])\n",
    "\n",
    "fullData[\"Test Item\"] = fullData[\"Test Item\"].astype('category')\n",
    "fullData[\"Test Item\"] = fullData[\"Test Item\"].cat.codes\n",
    "fullData[\"Library #3\"] = fullData[\"Library #3\"].astype('category')\n",
    "fullData[\"Library #3\"] = fullData[\"Library #3\"].cat.codes\n",
    "\n",
    "###################### Continuous normalized temp#######################################################\n",
    "fullData['Chip Temp'] = pd.to_numeric(fullData['Chip Temp'], errors='coerce').fillna(-40.0).astype(float)\n",
    "fullData['Chip Temp'] = fullData['Chip Temp'].map(lambda x: x/150.0)\n",
    "\n",
    "# create training and testing sets\n",
    "X = fullData.copy()\n",
    "X = shuffle(X)\n",
    "y = X[\"Shmoo Value\"]\n",
    "X.drop([\"Shmoo Value\"], axis = 1, inplace = True)\n",
    "\n",
    "X_train = X[:8196]\n",
    "y_train = y[:8196]\n",
    "X_test = X[-1500:]\n",
    "y_test = y[-1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.iloc[7])\n",
    "print(permanent.iloc[75100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permanent['Test Item'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(permanent.iloc[17765])\n",
    "print(permanent.iloc[2634])\n",
    "print(permanent.iloc[36227])\n",
    "print(permanent.iloc[54953])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomized search for hyperparameter optimization\n",
    "'''rf = RandomForestRegressor()\n",
    "pprint(rf.get_params())\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison of base model and randomsearch result\n",
    "'''base_model = RandomForestRegressor(random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "y_predicted_base = base_model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_predicted_base))\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "y_predicted_random = best_random.predict(X_test)\n",
    "print(mean_squared_error(y_test, y_predicted_random))\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"For the base model:\")\n",
    "    print(\"The test label is \" + str(y_test.iloc[i]) + \" and the predicted value is \" + str(y_predicted_base[i]))\n",
    "    print(\"For the randomized search optimal model:\")\n",
    "    print(\"The test label is still \" + str(y_test.iloc[i]) + \" and the predicted value is \" + str(y_predicted_random[i]))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is just some bs, forget about it\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [5],\n",
    "    'min_samples_split': [4],\n",
    "    'n_estimators': [200]\n",
    "}\n",
    "\n",
    "rfGrid = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(estimator = rfGrid, param_grid = param_grid, \n",
    "                          cv = 6, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "y_predicted_grid = best_grid.predict(X_test)\n",
    "feature_importances = pd.DataFrame(best_grid.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(feature_importances)\n",
    "ax = feature_importances.plot.bar(rot=65)\n",
    "plt.title(\"Feature importance in Random Forest\")\n",
    "plt.show()\n",
    "plt.scatter(y_test, y_predicted_grid,  color='green', alpha=0.1)\n",
    "plt.xlim([0.2,0.5])\n",
    "plt.ylim([0.2,0.5])\n",
    "plt.plot([0.2,0.5],[0.2,0.5], color='black')\n",
    "plt.xlabel(\"Test Vmin\")\n",
    "plt.ylabel(\"Predicted Vmin\")\n",
    "plt.title(\"Standard Cell Vmin Prediction Error\")\n",
    "plt.show()\n",
    "\n",
    "print(mean_absolute_error(y_test, y_predicted_grid))\n",
    "print(mean_absolute_error(y_test, y_predicted_grid)/y_test.mean())\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(best_grid, 'RFVminBasic.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
