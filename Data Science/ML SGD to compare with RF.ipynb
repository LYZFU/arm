{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Index</th>\n",
       "      <th>Chip Type</th>\n",
       "      <th>Chip Temp</th>\n",
       "      <th>Test Number</th>\n",
       "      <th>Test Item</th>\n",
       "      <th>Library #1</th>\n",
       "      <th>Library #2</th>\n",
       "      <th>Library #3</th>\n",
       "      <th>VDD (Range)</th>\n",
       "      <th>DVDD (Range)</th>\n",
       "      <th>Period (Range)</th>\n",
       "      <th>Result</th>\n",
       "      <th>Shmoo Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>TT</td>\n",
       "      <td>25</td>\n",
       "      <td>600</td>\n",
       "      <td>sc7p5mcpp96p</td>\n",
       "      <td>sfk</td>\n",
       "      <td>lvt</td>\n",
       "      <td>c16</td>\n",
       "      <td>0.495 V</td>\n",
       "      <td>0.900 V</td>\n",
       "      <td>100.00 ns</td>\n",
       "      <td>(P)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>TT</td>\n",
       "      <td>25</td>\n",
       "      <td>601</td>\n",
       "      <td>sc7p5mcpp96p</td>\n",
       "      <td>sfk</td>\n",
       "      <td>lvt</td>\n",
       "      <td>c18</td>\n",
       "      <td>0.495 V</td>\n",
       "      <td>0.900 V</td>\n",
       "      <td>100.00 ns</td>\n",
       "      <td>(P)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>TT</td>\n",
       "      <td>25</td>\n",
       "      <td>602</td>\n",
       "      <td>sc7p5mcpp96p</td>\n",
       "      <td>sfk</td>\n",
       "      <td>lvt</td>\n",
       "      <td>c20</td>\n",
       "      <td>0.495 V</td>\n",
       "      <td>0.900 V</td>\n",
       "      <td>100.00 ns</td>\n",
       "      <td>(P)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>TT</td>\n",
       "      <td>25</td>\n",
       "      <td>603</td>\n",
       "      <td>sc7p5mcpp96p</td>\n",
       "      <td>sfk</td>\n",
       "      <td>lvt</td>\n",
       "      <td>c24</td>\n",
       "      <td>0.495 V</td>\n",
       "      <td>0.900 V</td>\n",
       "      <td>100.00 ns</td>\n",
       "      <td>(P)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>TT</td>\n",
       "      <td>25</td>\n",
       "      <td>604</td>\n",
       "      <td>sc7p5mcpp96p</td>\n",
       "      <td>sfk</td>\n",
       "      <td>svt</td>\n",
       "      <td>c16</td>\n",
       "      <td>0.495 V</td>\n",
       "      <td>0.900 V</td>\n",
       "      <td>100.00 ns</td>\n",
       "      <td>(P)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File Index Chip Type Chip Temp  Test Number     Test Item Library #1  \\\n",
       "0         101        TT        25          600  sc7p5mcpp96p        sfk   \n",
       "1         101        TT        25          601  sc7p5mcpp96p        sfk   \n",
       "2         101        TT        25          602  sc7p5mcpp96p        sfk   \n",
       "3         101        TT        25          603  sc7p5mcpp96p        sfk   \n",
       "4         101        TT        25          604  sc7p5mcpp96p        sfk   \n",
       "\n",
       "  Library #2 Library #3 VDD (Range) DVDD (Range) Period (Range) Result  \\\n",
       "0        lvt        c16     0.495 V      0.900 V      100.00 ns    (P)   \n",
       "1        lvt        c18     0.495 V      0.900 V      100.00 ns    (P)   \n",
       "2        lvt        c20     0.495 V      0.900 V      100.00 ns    (P)   \n",
       "3        lvt        c24     0.495 V      0.900 V      100.00 ns    (P)   \n",
       "4        svt        c16     0.495 V      0.900 V      100.00 ns    (P)   \n",
       "\n",
       "  Shmoo Value  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML\n",
    "#import sklearn\n",
    "#sklearn.__version__# use this when reopening previously saved ML models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    #pprint allows us to see current hyperparameter values of our model \n",
    "from pprint import pprint\n",
    "# import data\n",
    "fullData = pd.read_csv(\"vminStd.csv\")\n",
    "permanent = pd.read_csv(\"vminStd.csv\")\n",
    "fullData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep rows with Vmin value, and delete unuseful columns\n",
    "fullData.dropna(subset=['Shmoo Value'], inplace=True)\n",
    "fullData.drop([\"VDD (Range)\", \"Library #1\", \"DVDD (Range)\", \"Period (Range)\", \"Result\", \"Test Number\"], axis = 1, inplace = True)\n",
    "##########################################################################################\n",
    "# DROP FILE INDEX COLUMN FOR NOW, ADD THE DATA LATER TO INCLUDE LEAKAGE ETC IN THE MODEL.\n",
    "fullData.drop([\"File Index\"], axis = 1, inplace = True)\n",
    "##########################################################################################\n",
    "\n",
    "# Change strings to integers, get rid of 'V' for voltages, etc...\n",
    "fullData['Shmoo Value'] = fullData['Shmoo Value'].map(lambda x: x.rstrip('V'))\n",
    "fullData[\"Shmoo Value\"] = fullData[\"Shmoo Value\"].astype('float')\n",
    "\n",
    "# Use this to decide between one-hot encoding and categorical encoding (or else)\n",
    "'''print(\"Data Columns and Different Values: \")\n",
    "print(fullData.columns)\n",
    "print()\n",
    "for column in fullData.columns:\n",
    "    print(fullData[str(column)].unique())\n",
    "'''\n",
    "# one hot encode categorical values. See bookmarks for why one hot rather than else\n",
    "fullData = pd.get_dummies(fullData, columns=[\"Chip Type\", \"Library #2\"])\n",
    "\n",
    "fullData[\"Test Item\"] = fullData[\"Test Item\"].astype('category')\n",
    "fullData[\"Test Item\"] = fullData[\"Test Item\"].cat.codes\n",
    "fullData[\"Library #3\"] = fullData[\"Library #3\"].astype('category')\n",
    "fullData[\"Library #3\"] = fullData[\"Library #3\"].cat.codes\n",
    "\n",
    "###################### Continuous normalized temp#######################################################\n",
    "fullData['Chip Temp'] = pd.to_numeric(fullData['Chip Temp'], errors='coerce').fillna(-40.0).astype(float)\n",
    "fullData['Chip Temp'] = fullData['Chip Temp'].map(lambda x: x/150.0)\n",
    "\n",
    "# create training and testing sets\n",
    "X = fullData.copy()\n",
    "X = shuffle(X)\n",
    "y = X[\"Shmoo Value\"]\n",
    "X.drop([\"Shmoo Value\"], axis = 1, inplace = True)\n",
    "\n",
    "X_train = X[:8196]\n",
    "y_train = y[:8196]\n",
    "X_test = X[-1500:]\n",
    "y_test = y[-1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Chip Temp  Test Item  Library #3  Chip Type_FF  Chip Type_FS  \\\n",
      "17756   0.166667          1           0             0             0   \n",
      "2634    1.000000          1           2             0             0   \n",
      "36227   1.000000          0           3             1             0   \n",
      "54953  -0.266667          1           1             0             1   \n",
      "67913   0.166667          1           1             0             0   \n",
      "\n",
      "       Chip Type_SF  Chip Type_SS  Chip Type_TT  Library #2_lvt  \\\n",
      "17756             0             0             1               0   \n",
      "2634              0             0             1               0   \n",
      "36227             0             0             0               0   \n",
      "54953             0             0             0               0   \n",
      "67913             1             0             0               0   \n",
      "\n",
      "       Library #2_svt  Library #2_ulvt  \n",
      "17756               0                1  \n",
      "2634                1                0  \n",
      "36227               0                1  \n",
      "54953               1                0  \n",
      "67913               1                0  \n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001,\n",
      " 'average': False,\n",
      " 'early_stopping': False,\n",
      " 'epsilon': 0.1,\n",
      " 'eta0': 0.01,\n",
      " 'fit_intercept': True,\n",
      " 'l1_ratio': 0.15,\n",
      " 'learning_rate': 'invscaling',\n",
      " 'loss': 'squared_loss',\n",
      " 'max_iter': 1000,\n",
      " 'n_iter_no_change': 5,\n",
      " 'penalty': 'l2',\n",
      " 'power_t': 0.25,\n",
      " 'random_state': None,\n",
      " 'shuffle': True,\n",
      " 'tol': 0.001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "{'alpha': array([1.e-01, 1.e-02, 1.e-03, 1.e-04, 1.e-05, 1.e-06]),\n",
      " 'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
      " 'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n",
      " 'penalty': ['l2', 'l1', 'elasticnet']}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-06, score=0.931, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-06, score=0.929, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-05, score=0.928, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-05, score=0.926, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.3s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-05, score=0.931, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.3s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-05, score=0.929, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.3s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.4s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.001, score=0.913, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.4s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.001, score=0.920, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.4s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.001, score=0.913, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.0001 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.896, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.0001 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.912, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.0001 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.856, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.6s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.894, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.6s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.902, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    0.6s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.889, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    0.6s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.0001, score=0.931, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.0001, score=0.929, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01, score=0.624, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01, score=0.505, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    0.8s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01, score=0.594, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.8s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.0001, score=0.931, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.8s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.0001, score=0.929, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    0.9s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    0.9s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=1e-06, score=-1539622538640751535455207424.000, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    1.2s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=1e-06, score=-1031384606622119658057105408.000, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    1.6s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=1e-06 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=1e-06, score=-1237418333678829815178723328.000, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    1.9s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.1, score=-0.000, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:    1.9s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.1, score=-0.000, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:    1.9s remaining:    0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.1, score=-0.002, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    2.0s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=0.0001 ....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=0.0001, score=0.928, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:    2.0s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=0.0001 ....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:    2.0s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=0.0001 ....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=0.0001, score=0.926, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:    2.0s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1, score=0.268, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1, score=0.291, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1, score=0.262, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.001 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.001, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.001 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.001, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.001 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.001, score=0.925, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=0.403, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=0.390, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=0.336, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=0.01 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=0.01, score=0.926, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=0.01 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=0.01, score=0.927, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:    2.3s remaining:    0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=0.01 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=0.01, score=0.924, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:    2.3s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06, score=0.715, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:    2.3s remaining:    0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06, score=0.775, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06, score=0.700, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.0001, score=0.931, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.0001, score=0.929, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.841, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.352, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.514, total=   0.1s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1, score=-0.074, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1, score=-0.048, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.1, score=-0.065, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=invscaling, alpha=0.001 ...\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=invscaling, alpha=0.001, score=0.931, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=invscaling, alpha=0.001 ...\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=invscaling, alpha=0.001, score=0.929, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=invscaling, alpha=0.001 ...\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=invscaling, alpha=0.001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.1, score=0.856, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.1, score=0.855, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.1, score=0.854, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.1 ........\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.1, score=0.860, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.1 ........\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.1, score=0.862, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.1 ........\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.1, score=0.859, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.1 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-116.812, total=   0.2s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.1 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-3429.009, total=   0.6s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.1 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-390924.075, total=   0.7s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.01 ......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.01, score=0.519, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.01 ......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.01, score=0.529, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.01 ......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.01, score=0.483, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.931, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.929, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.477, total=   0.1s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.051, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.722, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.001, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.001, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.001, score=0.926, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.677, total=   0.1s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.493, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.568, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.0001, score=0.676, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.0001, score=0.519, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.0001, score=0.561, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=1e-06 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=1e-06, score=0.928, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=1e-06 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=1e-06 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=1e-06, score=0.926, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001, score=0.677, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001, score=0.512, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001, score=0.539, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.650, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.565, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.720, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06, score=-0.217, total=   0.2s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06, score=-707.059, total=   0.2s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06, score=-1010.421, total=   0.2s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.1, score=0.146, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.1, score=0.136, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=0.1, score=0.100, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-05, score=-14.266, total=   0.1s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-05, score=-1037.511, total=   0.1s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-05, score=-19.909, total=   0.1s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.01, score=0.929, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.01, score=0.926, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=0.01, score=0.925, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01, score=0.537, total=   0.1s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01, score=0.566, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01, score=0.347, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.001, score=0.612, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.001, score=0.744, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.001, score=0.782, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.0001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.0001, score=-223410333555109.750, total=   0.2s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.0001, score=-5362873658202.908, total=   0.2s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.0001, score=-10216028166558.959, total=   0.2s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=0.120, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=0.084, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=0.024, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=1e-06, score=0.928, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=1e-06, score=0.926, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.0001, score=0.926, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=0.206, total=   0.1s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=-5.146, total=   0.1s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.0001, score=-1.162, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.676, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.518, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.559, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.01, score=-144443800736863.219, total=   0.5s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.01, score=-4842692040467532.000, total=   0.6s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.01 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=optimal, alpha=0.01, score=-3998091422962252.500, total=   0.5s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.0001, score=0.928, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.0001, score=0.926, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.1 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.1, score=-0.001, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.1 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.1, score=-0.001, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.1 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.1, score=-0.000, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-06, score=-1316122080029854214402342912.000, total=   0.5s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-06, score=-169825048585943691707809792.000, total=   0.5s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-06, score=-1812710488339865958600409088.000, total=   0.4s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.001 ......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.001, score=0.920, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.001 ......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.001, score=0.921, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.001 ......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.001, score=0.919, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05, score=0.764, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05, score=0.512, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05, score=0.767, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-208.727, total=   0.4s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-13258.807, total=   1.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sv116/my_project_dir/my_project_env/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:1185: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-1198592.467, total=   0.9s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.001, score=-6722041770099811.000, total=   0.4s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.001, score=-317903288270192.688, total=   0.9s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=0.001, score=-5014064297024190.000, total=   0.6s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.0001 ....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.0001 ....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.0001 ....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.0001, score=0.926, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.1, score=0.355, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.1, score=0.757, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.1, score=0.254, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.926, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.924, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.001 ...\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.001, score=0.923, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.001 ...\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.001, score=0.921, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.001 ...\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=0.001, score=0.919, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06, score=0.713, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06, score=0.776, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-06, score=0.698, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.931, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.929, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.01, score=0.895, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.01, score=0.901, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.01, score=0.896, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.1 .......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.1, score=-0.007, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.1 .......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.1, score=-0.000, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=0.1 .......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=0.1, score=-0.004, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=1e-06, score=0.928, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=constant, alpha=1e-06, score=0.926, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.01, score=0.894, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.01, score=0.902, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.01, score=0.889, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.931, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.929, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=1e-05, score=0.928, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=1e-05, score=0.926, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.001, score=0.929, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.001, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.001, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.1 .\n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-0.000, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.1 .\n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-0.012, total=   0.1s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.1 .\n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.1, score=-4.495, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.0001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.900, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.798, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.807, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05, score=0.716, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05, score=0.774, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=1e-05, score=0.702, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.1, score=0.860, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.1, score=0.869, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.1 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=constant, alpha=0.1, score=0.845, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=1e-05 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=1e-05, score=-37.676, total=   0.1s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=1e-05 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=1e-05, score=-49.080, total=   0.1s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=1e-05 ......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=1e-05, score=-36.213, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=1e-05 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=1e-05, score=0.928, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=1e-05 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=constant, alpha=1e-05 .....\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=constant, alpha=1e-05, score=0.926, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.001, score=0.857, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.001, score=0.908, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=0.001, score=0.913, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01, score=0.258, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01, score=0.317, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=optimal, alpha=0.01, score=0.319, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.0001, score=0.928, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.0001, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.0001 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=constant, alpha=0.0001, score=0.926, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.520, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.530, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=0.01, score=0.483, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.01 .......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.01, score=0.497, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.01 .......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.01, score=0.506, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=optimal, alpha=0.01 .......\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=optimal, alpha=0.01, score=0.488, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.931, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.929, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=-0.057, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=-0.047, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.1, score=-0.061, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.1, score=-0.001, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.1, score=-0.001, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.1 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=invscaling, alpha=0.1, score=-0.000, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-05, score=0.874, total=   0.1s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-05, score=0.895, total=   0.1s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=optimal, alpha=1e-05, score=0.912, total=   0.2s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001, score=0.489, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001, score=0.454, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.001, score=0.349, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.01 .......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.01, score=0.929, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.01 .......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.01, score=0.928, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.01 .......\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.01, score=0.926, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.0001 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.619, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.0001 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.907, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=optimal, alpha=0.0001 .....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  penalty=l2, loss=huber, learning_rate=optimal, alpha=0.0001, score=0.897, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.677, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.519, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-05, score=0.560, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=1e-06 ...\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=1e-06, score=0.931, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=1e-06 ...\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=1e-06, score=0.929, total=   0.0s\n",
      "[CV] penalty=l1, loss=huber, learning_rate=invscaling, alpha=1e-06 ...\n",
      "[CV]  penalty=l1, loss=huber, learning_rate=invscaling, alpha=1e-06, score=0.927, total=   0.1s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-06, score=0.928, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=constant, alpha=1e-06, score=0.926, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.01, score=0.899, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.01, score=0.898, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=0.01, score=0.896, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=1e-05, score=0.801, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=1e-05, score=0.441, total=   0.1s\n",
      "[CV] penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=huber, learning_rate=optimal, alpha=1e-05, score=-2.145, total=   0.1s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.239, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.458, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.296, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.001, score=0.923, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.001, score=0.921, total=   0.0s\n",
      "[CV] penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.001 \n",
      "[CV]  penalty=l1, loss=squared_loss, learning_rate=invscaling, alpha=0.001, score=0.919, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.01, score=0.616, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.01, score=0.645, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.01 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=constant, alpha=0.01, score=0.547, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=1e-05 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=1e-05, score=0.928, total=   0.1s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=1e-05 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[CV] penalty=l2, loss=huber, learning_rate=constant, alpha=1e-05 .....\n",
      "[CV]  penalty=l2, loss=huber, learning_rate=constant, alpha=1e-05, score=0.926, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=1e-05, score=-13205667666118.768, total=   0.4s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=1e-05, score=-37367427665.645, total=   0.5s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=1e-05 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=optimal, alpha=1e-05, score=-8260484081430.979, total=   0.3s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.931, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.929, total=   0.0s\n",
      "[CV] penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-05 \n",
      "[CV]  penalty=l2, loss=squared_loss, learning_rate=invscaling, alpha=1e-05, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.381, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.534, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=constant, alpha=0.001, score=0.313, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-06, score=0.677, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-06, score=0.518, total=   0.0s\n",
      "[CV] penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=l1, loss=epsilon_insensitive, learning_rate=invscaling, alpha=1e-06, score=0.559, total=   0.0s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06, score=-451.972, total=   0.4s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06, score=-567.523, total=   0.3s\n",
      "[CV] penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06 \n",
      "[CV]  penalty=l2, loss=epsilon_insensitive, learning_rate=optimal, alpha=1e-06, score=-159.926, total=   0.4s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.931, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.929, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-06 \n",
      "[CV]  penalty=elasticnet, loss=squared_loss, learning_rate=invscaling, alpha=1e-06, score=0.927, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01, score=0.379, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01, score=0.342, total=   0.0s\n",
      "[CV] penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01 \n",
      "[CV]  penalty=elasticnet, loss=epsilon_insensitive, learning_rate=invscaling, alpha=0.01, score=0.350, total=   0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   22.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'penalty': 'l1',\n",
       " 'loss': 'squared_loss',\n",
       " 'learning_rate': 'invscaling',\n",
       " 'alpha': 1e-06}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = linear_model.SGDRegressor()\n",
    "pprint(rf.get_params())\n",
    "\n",
    "\n",
    "random_grid = {\n",
    "    'alpha': 10.0 ** -np.arange(1, 7),\n",
    "    'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "}\n",
    "pprint(random_grid)\n",
    "\n",
    "\n",
    "rf = linear_model.SGDRegressor(random_state=42)\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=50, random_state=42)\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010834643627530052\n",
      "0.010834610355475989\n",
      "For the base model:\n",
      "The test label is 0.29 and the predicted value is 0.2850391246433185\n",
      "For the randomized search optimal model:\n",
      "The test label is still 0.29 and the predicted value is 0.2850408131066344\n",
      "For the base model:\n",
      "The test label is 0.395 and the predicted value is 0.394411368687573\n",
      "For the randomized search optimal model:\n",
      "The test label is still 0.395 and the predicted value is 0.39444550800717776\n",
      "For the base model:\n",
      "The test label is 0.225 and the predicted value is 0.22176488145922602\n",
      "For the randomized search optimal model:\n",
      "The test label is still 0.225 and the predicted value is 0.22174459206980124\n",
      "For the base model:\n",
      "The test label is 0.31 and the predicted value is 0.3261234053675829\n",
      "For the randomized search optimal model:\n",
      "The test label is still 0.31 and the predicted value is 0.326143055376632\n",
      "For the base model:\n",
      "The test label is 0.325 and the predicted value is 0.3430302758450094\n",
      "For the randomized search optimal model:\n",
      "The test label is still 0.325 and the predicted value is 0.3430667034473455\n"
     ]
    }
   ],
   "source": [
    "base_model = linear_model.SGDRegressor(random_state = 42)\n",
    "base_model.fit(X_train, y_train)\n",
    "y_predicted_base = base_model.predict(X_test)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_test, y_predicted_base))\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "y_predicted_random = best_random.predict(X_test)\n",
    "print(mean_absolute_error(y_test, y_predicted_random))\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"For the base model:\")\n",
    "    print(\"The test label is \" + str(y_test.iloc[i]) + \" and the predicted value is \" + str(y_predicted_base[i]))\n",
    "    print(\"For the randomized search optimal model:\")\n",
    "    print(\"The test label is still \" + str(y_test.iloc[i]) + \" and the predicted value is \" + str(y_predicted_random[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# forget about this\\nparam_grid = {\\n    'bootstrap': [True],\\n    'max_depth': [10],\\n    'max_features': ['auto'],\\n    'min_samples_leaf': [5],\\n    'min_samples_split': [4],\\n    'n_estimators': [200]\\n}\\n\\nrfGrid = RandomForestRegressor()\\ngrid_search = GridSearchCV(estimator = rfGrid, param_grid = param_grid, \\n                          cv = 6, verbose = 2)\\n\\ngrid_search.fit(X_train, y_train)\\ngrid_search.best_params_\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# forget about this\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [10],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [5],\n",
    "    'min_samples_split': [4],\n",
    "    'n_estimators': [200]\n",
    "}\n",
    "\n",
    "rfGrid = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(estimator = rfGrid, param_grid = param_grid, \n",
    "                          cv = 6, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010834610355475989\n",
      "0.03463971595203015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SGDVminBasic.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(y_test, y_predicted_random,  color='green', alpha=0.1)\n",
    "plt.xlim([0.2,0.5])\n",
    "plt.ylim([0.2,0.5])\n",
    "plt.plot([0.2,0.5],[0.2,0.5], color='black')\n",
    "plt.xlabel(\"Test Vmin\")\n",
    "plt.ylabel(\"Predicted Vmin\")\n",
    "plt.title(\"Standard Cell Vmin Prediction Error\")\n",
    "plt.show()\n",
    "\n",
    "print(mean_absolute_error(y_test, y_predicted_random))\n",
    "print(mean_absolute_error(y_test, y_predicted_random)/y_test.mean())\n",
    "\n",
    "from joblib import dump, load\n",
    "dump(best_random, 'SGDVminBasic.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
